;-*-Org-*-
* General Settings (README)
  Whilst RandomCopying can be solved analytically, payoff-weighted
  copying and copy-the-bests require a large number of
  simulation. Given this is computationally intensive (not
  prohibitive, but still much more than a coffee), the
  .RData file ./explorations/exploration1.RData contains all the
  result required for the preliminary analysis below. All figures are
  stored in the folder ./explorations/figures.
* Random Copying 
** Effect of variations in z (fig1)

    #+BEGIN_SRC R 
plot(1:150,RC(k=1:150,c=0,z=1),type="l",xlab="k",ylab="P(loss)",ylim=c(0,0.5),main=c("Random Copying with c=0"))
lines(1:150,RC(k=1:150,c=0,z=0.5),lty=2)
lines(1:150,RC(k=1:150,c=0,z=0.1),lty=3)
legend("topright",legend=c("z=1","z=0.5","z=0.1"),lty=1:3)
dev.print(device=pdf,"./explorations/figures/fig1.pdf")
dev.print(device=png,"./explorations/figures/fig1.png",width=400,height=400)

    #+END_SRC
   
   In all cases increase in k determines increase in the probability
   of loss, though this becomes stable around k=50. Decrease in z
   leads to decrease in the probability of trait loss.

** Effect of variation in c (fig2

    #+BEGIN_SRC R 
plot(1:150,RC(k=1:150,c=0,z=1),type="l",xlab="k",ylab="P(loss)",ylim=c(0,0.5),main="Random Copying with z=1")
lines(1:150,RC(k=1:150,c=0.001,z=1),lty=2)
lines(1:150,RC(k=1:150,c=0.01,z=1),lty=3)
legend("topright",legend=c("c=0","c=0.001","c=0.01"),lty=1:3)
dev.print(device=pdf,"./explorations/figures/fig2.pdf")
dev.print(device=png,"./explorations/figures/fig2.png",width=400,height=400)
    #+END_SRC
   
   The core consequence of c is that the increase of probability of
   loss determined by the increase in k is compensated by increased
   probability of convergent evolution. Thus with increasing k we have
   an increase probability of loss, which then start to decrease with
   different rates as a function of c.

* Payoff-Weighted Copying

   We execute 10,000 runs of the function PW(), which returns a
   TRUE/FALSE statement of whether the new trait is lost (TRUE) or
   retained (FALSE) at the population level. For simplicity we hold
   c=0, z=1, and g_j=1,in all cases. 

** Effect of difference in mean payoff (fig3)

    #+BEGIN_SRC R 
#Not Run:
#Set Multicore Processing For Faster Computation time
#library(utils)
#library(foreach)
#library(doParallel)
#registerDoParallel(cores=4)
#nsim=10000 #number of simulation for each value of k
load("./explorations/exploration1.RData")
#pLoss1.pw=foreach(k=1:150,.combine=c) %dopar% {sum(replicate(10000,PW(k=k,c=0,gi=1,gj=1,sigma=1,z=1)))/nsim}
#pLoss2.pw=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,PW(k=k,c=0,gi=2,gj=1,sigma=1,z=1)))/nsim}
#pLoss3.pw=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,PW(k=k,c=0,gi=3,gj=1,sigma=1,z=1)))/nsim}

#Actual Plot
plot(1:150,pLoss1.pw,type="l",xlab="k",ylab="P(loss)",ylim=c(0,0.5),main="Random Copying with Evaluation, z=1,c=0,sigma=1")
lines(1:150,pLoss2.pw,lty=2)
lines(1:150,pLoss3.pw,lty=3)
legend("topright",legend=c(expression(paste(g[i],"=1",sep="")),expression(paste(g[i],"=2",sep="")),expression(paste(g[i],"=3",sep=""))),lty=1:3)
dev.print(device=pdf,"./explorations/figures/fig3.pdf")
dev.print(device=png,"./explorations/figures/fig3.png",width=400,height=400)

    #+END_SRC
   
   Not surprisingly, an increase in the performance of g_i over g_j
   leads to higher chance of trait retention. Notice also that when
   g_i=g_j=1, we have the same probability as in the random copying
   model (cf fig2).  
   
** Effect of payoff uncertainty (fig4)

    #+BEGIN_SRC R 
#Not Run:
#Set Multicore Processing For Faster Computation time
#library(utils)
#library(foreach)
#library(doParallel)
#registerDoParallel(cores=4)
#nsim=10000 #number of simulation for each value of k
load("./explorations/exploration1.RData")
#pLoss1.pwS=pLoss2.pw
#pLoss2.pwS=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,PW(k=k,c=0,gi=2,gj=1,sigma=2,z=1)))/nsim}
#pLoss3.pwS=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,PW(k=k,c=0,gi=2,gj=1,sigma=3,z=1)))/nsim}

#Actual Plot
plot(1:150,pLoss1.pwS,type="l",xlab="k",ylab="P(loss)",ylim=c(0,0.5),main="Random Copying with Evaluation, z=1,c=0,gi=2,gj=1")
lines(1:150,pLoss2.pwS,lty=2)
lines(1:150,pLoss3.pwS,lty=3)
legend("topright",legend=c(expression(paste(sigma,"=1",sep="")),expression(paste(sigma,"=2",sep="")),expression(paste(sigma,"=3",sep=""))),lty=1:3)
dev.print(device=pdf,"./explorations/figures/fig4.pdf")
dev.print(device=png,"./explorations/figures/fig4.png",width=400,height=400)

    #+END_SRC

   An increase in payoff uncertainy leads to a higher probability of
   trait loss.

* Copy-the-Best

   We execute 10,000 runs of the function CB(), which returns a
   TRUE/FALSE statement of whether the new trait is lost (TRUE) or
   retained (FALSE) at the population level. For simplicity we hold
   c=0, z=1, and g_j=1,in all cases. 

** Effect of difference in mean payoff (fig5)


  #+BEGIN_SRC R 
#Not Run:
#Set Multicore Processing For Faster Computation time
#library(utils)
#library(foreach)
#library(doParallel)
#registerDoParallel(cores=4)
#nsim=10000 #number of simulation for each value of k
load("./explorations/exploration1.RData")
#pLoss1.cb=foreach(k=1:150,.combine=c) %dopar% {sum(replicate(10000,CB(k=k,c=0,gi=1,gj=1,sigma=1,z=1)))/nsim}
#pLoss2.cb=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,CB(k=k,c=0,gi=2,gj=1,sigma=1,z=1)))/nsim}
#pLoss3.cb=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,CB(k=k,c=0,gi=3,gj=1,sigma=1,z=1)))/nsim}

#Actual Plot
plot(1:150,pLoss1.cb,type="l",xlab="k",ylab="P(loss)",ylim=c(0.2,1),main="Copy-the-Best, c=0,z=1,gj=1,sigma=1")
lines(1:150,pLoss2.cb,lty=2)
lines(1:150,pLoss3.cb,lty=3)
legend("bottomright",legend=c(expression(paste(g[i],"=1",sep="")),expression(paste(g[i],"=2",sep="")),expression(paste(g[i],"=3",sep=""))),lty=1:3)
dev.print(device=pdf,"./explorations/figures/fig5.pdf")
dev.print(device=png,"./explorations/figures/fig5.png",width=400,height=400)
    #+END_SRC
   
   The decrease in the probability of loss caused by lower mean payoff
   of the innovator is the same as in the PW model. However while in
   PW an increase in g_i will simply change the asymptotic level of
   the probability loss, here we also see a continous determental
   effect of k over larger values. To put it simply, when we have
   payoff weighted copying, the effect of k is nullified after a
   certain size, while when we have copy-the-best, this has an effect
   in the system, as larger k implies higher chance of an extreme
   case of a payoff higher than the one observed by the innovator.


** Effect of payoff uncertainty (fig6)
 #+BEGIN_SRC R 
#Not Run:
#Set Multicore Processing For Faster Computation time
#library(utils)
#library(foreach)
#library(doParallel)
#registerDoParallel(cores=4)
#nsim=10000 #number of simulation for each value of k
load("./explorations/exploration1.RData")
#pLoss1.cbS=pLoss2.cb
#pLoss2.cbS=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,CB(k=k,c=0,gi=2,gj=1,sigma=2,z=1)))/nsim}
#pLoss3.cbS=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,CB(k=k,c=0,gi=2,gj=1,sigma=3,z=1)))/nsim}

#Actual Plot
plot(1:150,pLoss1.cbS,type="l",xlab="k",ylab="P(loss)",ylim=c(0.2,1),main="Copy-the-Best, c=0,z=1,gj=1,gi=2")
lines(1:150,pLoss2.cbS,lty=2)
lines(1:150,pLoss3.cbS,lty=3)
legend("right",legend=c(expression(paste(sigma,"=1",sep="")),expression(paste(sigma,"=2",sep="")),expression(paste(sigma,"=3",sep=""))),lty=1:3)
dev.print(device=pdf,"./explorations/figures/fig6.pdf")
dev.print(device=png,"./explorations/figures/fig6.png",width=400,height=400)

    #+END_SRC

   Here the effect of sigma is similar to the PWF, higher payoff
   uncertainty leads to a higher probability of trait loss. 

** Effect of difference in mean payoff and high convergent innovation (fig7a) 
 #+BEGIN_SRC R 
#Not Run:
#Set Multicore Processing For Faster Computation time
#library(utils)
#library(foreach)
#library(doParallel)
#registerDoParallel(cores=4)
#nsim=10000 #number of simulation for each value of k
load("./explorations/exploration1.RData")
#pLoss1.cbC=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,CB(k=k,c=0.01,gi=1,gj=1,sigma=1,z=1)))/nsim}
#pLoss2.cbC=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,CB(k=k,c=0.01,gi=2,gj=1,sigma=1,z=1)))/nsim}
#pLoss3.cbC=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,CB(k=k,c=0.01,gi=3,gj=1,sigma=1,z=1)))/nsim}

#Actual Plot
plot(1:150,pLoss1.cbC,type="l",xlab="k",ylab="P(loss)",ylim=c(0,0.8),main="Copy-the-Best, c=0.01,z=1,sigma=1,gj=1")
lines(1:150,pLoss2.cbC,lty=2)
lines(1:150,pLoss3.cbC,lty=3)
legend("topright",legend=c(expression(paste(g[i],"=1",sep="")),expression(paste(g[i],"=2",sep="")),expression(paste(g[i],"=3",sep=""))),lty=1:3)
dev.print(device=pdf,"./explorations/figures/fig7a.pdf")
dev.print(device=png,"./explorations/figures/fig7a.png",width=400,height=400)

    #+END_SRC
** Effect of difference in mean payoff and low convergent innovation (fig7b) 
 #+BEGIN_SRC R 
#Not Run:
#Set Multicore Processing For Faster Computation time
#library(utils)
#library(foreach)
#library(doParallel)
#registerDoParallel(cores=4)
#nsim=10000 #number of simulation for each value of k
load("./explorations/exploration1.RData")
#pLoss1.cbClo=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,CB(k=k,c=0.001,gi=1,gj=1,sigma=1,z=1)))/nsim}
#pLoss2.cbClo=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,CB(k=k,c=0.001,gi=2,gj=1,sigma=1,z=1)))/nsim}
#pLoss3.cbClo=foreach(k=1:150, .combine=c) %dopar% {sum(replicate(10000,CB(k=k,c=0.001,gi=3,gj=1,sigma=1,z=1)))/nsim}

#Actual Plot
plot(1:150,pLoss1.cbClo,type="l",xlab="k",ylab="P(loss)",ylim=c(0,1),main="Copy-the-Best, c=0.001,z=1,sigma=1,gj=1")
lines(1:150,pLoss2.cbClo,lty=2)
lines(1:150,pLoss3.cbClo,lty=3)
legend("bottomright",legend=c(expression(paste(g[i],"=1",sep="")),expression(paste(g[i],"=2",sep="")),expression(paste(g[i],"=3",sep=""))),lty=1:3)
dev.print(device=pdf,"./explorations/figures/fig7b.pdf")
dev.print(device=png,"./explorations/figures/fig7b.png",width=400,height=400)

    #+END_SRC
